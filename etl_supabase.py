"""
ETL pipeline with Supabase + VNStock (REST API version for GitHub Actions)

- Extract: 3 bÃ¡o cÃ¡o tÃ i chÃ­nh FPT tá»« VNStock
- Transform: chuáº©n hÃ³a & lÆ°u CSV
- Load: upsert vÃ o Supabase báº±ng REST API (supabase.table)
- Upload: Ä‘áº©y CSV lÃªn Supabase Storage (bucket: processed-data)
"""

import os
import pandas as pd
from vnstock import Vnstock
from supabase import create_client, Client

# ====== CONFIG ======

# URL project Supabase
SUPABASE_URL = "https://fxjrsxepzrbpmqygfvee.supabase.co"

# Láº¥y SERVICE KEY tá»« biáº¿n mÃ´i trÆ°á»ng náº¿u cÃ³ (GitHub Actions),
# náº¿u khÃ´ng thÃ¬ fallback vá» giÃ¡ trá»‹ báº¡n hard-code cho cháº¡y local.
SUPABASE_SERVICE_KEY = os.getenv(
    "SUPABASE_SERVICE_KEY",
    "sb_secret_qzMFzF85u7PxwvJmTVHooQ_Q9Tj7Zf9"
)

# ====== HÃ€M CHÃNH ======
def run_etl():
    print("ðŸ”¹ Extract: dÃ¹ng VNStock Ä‘á»ƒ láº¥y bÃ¡o cÃ¡o tÃ i chÃ­nh FPT...")

    stock = Vnstock().stock(symbol="FPT", source="VCI")

    # 1) Income Statement (KQKD)
    income_df = stock.finance.income_statement(period="year", lang="vi", dropna=True)

    # 2) Balance Sheet (BCÄKT)
    balance_df = stock.finance.balance_sheet(period="year", lang="vi", dropna=True)

    # 3) Cash Flow (LCTT)
    cashflow_df = stock.finance.cash_flow(period="year", dropna=True)

    print("âž¡ Income Statement sample:")
    print(income_df.head())

    # ====== TRANSFORM ======
    print("ðŸ”¹ Transform: chuáº©n hÃ³a dá»¯ liá»‡u ...")

    # ThÃªm cá»™t ticker náº¿u thiáº¿u
    for df in (income_df, balance_df, cashflow_df):
        if "ticker" not in df.columns:
            df["ticker"] = "FPT"

    # LÆ°u 3 file CSV
    income_df.to_csv("income_statement.csv", index=False)
    balance_df.to_csv("balance_sheet.csv", index=False)
    cashflow_df.to_csv("cash_flow.csv", index=False)
    print("ÄÃ£ lÆ°u 3 file CSV.")

    # ====== LOAD: Supabase REST API qua supabase-py ======
    print("ðŸ”¹ Load: upsert dá»¯ liá»‡u vÃ o Supabase qua REST API ...")

    supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)

    # Chuyá»ƒn DataFrame -> list[dict]
    income_data = income_df.to_dict(orient="records")
    balance_data = balance_df.to_dict(orient="records")
    cashflow_data = cashflow_df.to_dict(orient="records")

    # LÆ°u Ã½:
    # - Báº£ng trong Supabase pháº£i tá»“n táº¡i sáºµn:
    #   fpt_income_statement, fpt_balance_sheet, fpt_cash_flow
    # - NÃªn táº¡o PRIMARY KEY hoáº·c UNIQUE Ä‘á»ƒ upsert hoáº¡t Ä‘á»™ng Ä‘Ãºng.
    resp1 = supabase.table("fpt_income_statement").upsert(income_data).execute()
    print("Upsert fpt_income_statement:", resp1)

    resp2 = supabase.table("fpt_balance_sheet").upsert(balance_data).execute()
    print("Upsert fpt_balance_sheet:", resp2)

    resp3 = supabase.table("fpt_cash_flow").upsert(cashflow_data).execute()
    print("Upsert fpt_cash_flow:", resp3)

    # ====== UPLOAD CSV LÃŠN STORAGE ======
    print("ðŸ”¹ Upload 3 file CSV lÃªn bucket processed-data ...")

    files = [
        ("income_statement.csv", "income_statement.csv"),
        ("balance_sheet.csv", "balance_sheet.csv"),
        ("cash_flow.csv", "cash_flow.csv"),
    ]

    for local, remote in files:
        with open(local, "rb") as f:
            res = supabase.storage.from_("processed-data").upload(remote, f)
            print(f"Uploaded {local}:", res)

    print("âœ… ETL hoÃ n táº¥t!")

# ====== ENTRYPOINT ======
if __name__ == "__main__":
    run_etl()
