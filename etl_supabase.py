# """
# ETL pipeline with Supabase REST API + VNStock:
# - Extract 3 b√°o c√°o t√†i ch√≠nh t·ª´ VNStock
# - Transform: pack d·ªØ li·ªáu v√†o c·ªôt data (JSONB)
# - Load v√†o Supabase PostgreSQL qua REST API
# """

# import os
# import json
# import pandas as pd
# import requests
# from vnstock import Vnstock

# SUPABASE_URL = os.getenv("SUPABASE_URL", "https://tzwepclhllftfmoeimjd.supabase.co")
# SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

# if not SUPABASE_SERVICE_KEY:
#     raise RuntimeError("Thi·∫øu SUPABASE_SERVICE_KEY trong ENV")

# REST_BASE_URL = f"{SUPABASE_URL}/rest/v1"
# STORAGE_BASE_URL = f"{SUPABASE_URL}/storage/v1"

# COMMON_HEADERS = {
#     "apikey": SUPABASE_SERVICE_KEY,
#     "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
# }

# JSON_HEADERS = {
#     **COMMON_HEADERS,
#     "Content-Type": "application/json",
#     "Prefer": "return=minimal"
# }


# def df_to_jsonb_records(df: pd.DataFrame):
#     """
#     Convert DataFrame to records with JSONB format.
#     Gi·∫£ s·ª≠ DataFrame c√≥ c·ªôt 'NƒÉm' ho·∫∑c 'Year', c√°c c·ªôt kh√°c pack v√†o 'data'.
#     """
#     records = []
    
#     # T√¨m c·ªôt nƒÉm (NƒÉm ho·∫∑c Year)
#     year_col = None
#     for col in df.columns:
#         if col.lower() in ['nƒÉm', 'year']:
#             year_col = col
#             break
    
#     for _, row in df.iterrows():
#         year = int(row[year_col]) if year_col and pd.notna(row[year_col]) else None
        
#         # Pack to√†n b·ªô d·ªØ li·ªáu v√†o JSONB
#         data_dict = {}
#         for col in df.columns:
#             if col.lower() not in ['nƒÉm', 'year', 'cp', 'ticker']:
#                 val = row[col]
#                 data_dict[col] = None if pd.isna(val) else val
        
#         ticker = row.get('CP') or row.get('ticker', 'FPT')
        
#         record = {
#             "ticker": ticker,
#             "year": year,
#             "data": data_dict
#         }
#         records.append(record)
    
#     return records


# def upsert_table(records: list, table_name: str, chunk_size: int = 300):
#     """G·ª≠i d·ªØ li·ªáu l√™n Supabase REST API theo t·ª´ng chunk."""
#     print(f"üîπ Upsert {len(records)} rows v√†o b·∫£ng {table_name} qua REST API...")

#     url = f"{REST_BASE_URL}/{table_name}"

#     for i in range(0, len(records), chunk_size):
#         chunk = records[i:i + chunk_size]
#         resp = requests.post(url, headers=JSON_HEADERS, data=json.dumps(chunk))
#         if not resp.ok:
#             print(f"‚ùå L·ªói khi upsert chunk {i}-{i+len(chunk)} v√†o {table_name}: {resp.status_code}")
#             print(resp.text)
#             resp.raise_for_status()
#         else:
#             print(f"‚úÖ ƒê√£ upsert {len(chunk)} rows v√†o {table_name}")


# def upload_to_storage(local_path: str, remote_path: str, bucket: str = "processed-data"):
#     """Upload file l√™n Supabase Storage qua REST API."""
#     url = f"{STORAGE_BASE_URL}/object/{bucket}/{remote_path}"
#     params = {"upsert": "true"}

#     ext = os.path.splitext(local_path)[1].lower()
#     content_type = "text/csv" if ext == ".csv" else "application/octet-stream"

#     headers = {
#         **COMMON_HEADERS,
#         "Content-Type": content_type,
#     }

#     with open(local_path, "rb") as f:
#         resp = requests.post(url, headers=headers, params=params, data=f)
#         if not resp.ok:
#             print(f"‚ùå L·ªói upload {local_path} -> {bucket}/{remote_path}: {resp.status_code}")
#             print(resp.text)
#             resp.raise_for_status()
#         else:
#             print(f"‚úÖ Uploaded {local_path} -> {bucket}/{remote_path}")


# def run_etl():
#     # 1) EXTRACT
#     print("üîπ Extract: d√πng VNStock ƒë·ªÉ l·∫•y b√°o c√°o t√†i ch√≠nh FPT...")
    
#     stock = Vnstock().stock(symbol="FPT", source="VCI")
    
#     income_df = stock.finance.income_statement(period="year", lang="vi", dropna=True)
#     balance_df = stock.finance.balance_sheet(period="year", lang="vi", dropna=True)
#     cashflow_df = stock.finance.cash_flow(period="year", dropna=True)

#     print("‚û° Income Statement sample:")
#     print(income_df.head())
#     print(f"Columns: {income_df.columns.tolist()}")

#     # 2) TRANSFORM
#     print("üîπ Transform: chu·∫©n h√≥a d·ªØ li·ªáu...")
    
#     income_records = df_to_jsonb_records(income_df)
#     balance_records = df_to_jsonb_records(balance_df)
#     cashflow_records = df_to_jsonb_records(cashflow_df)
    
#     print(f"‚úÖ Converted {len(income_records)} income records")
#     print(f"‚úÖ Converted {len(balance_records)} balance records")
#     print(f"‚úÖ Converted {len(cashflow_records)} cashflow records")
    
#     print(f"\nüìã Sample income record: {json.dumps(income_records[0], ensure_ascii=False, indent=2)}")

#     # L∆∞u CSV (original format)
#     income_df.to_csv("income_statement.csv", index=False)
#     balance_df.to_csv("balance_sheet.csv", index=False)
#     cashflow_df.to_csv("cash_flow.csv", index=False)
#     print("‚úÖ ƒê√£ l∆∞u 3 file CSV.")

#     # 3) LOAD ‚Üí Supabase qua REST API
#     upsert_table(income_records, "fpt_income_statement")
#     upsert_table(balance_records, "fpt_balance_sheet")
#     upsert_table(cashflow_records, "fpt_cash_flow")

#     print("‚úÖ ƒê√£ g·ª≠i d·ªØ li·ªáu l√™n 3 b·∫£ng qua REST API.")

#     # 4) UPLOAD CSV ‚Üí STORAGE
#     print("üîπ Upload 3 file CSV l√™n bucket processed-data...")

#     upload_to_storage("income_statement.csv", "income_statement.csv")
#     upload_to_storage("balance_sheet.csv", "balance_sheet.csv")
#     upload_to_storage("cash_flow.csv", "cash_flow.csv")

#     print("‚úÖ ETL ho√†n t·∫•t!")


# if __name__ == "__main__":
#     run_etl()
"""
ETL pipeline with Supabase REST API + VNStock:
- Extract 3 b√°o c√°o t√†i ch√≠nh t·ª´ VNStock
- Transform: flatten d·ªØ li·ªáu, m·ªói c·ªôt t√†i ch√≠nh th√†nh 1 column
- Load v√†o Supabase PostgreSQL qua REST API
"""

import os
import json
import pandas as pd
import requests
from vnstock import Vnstock

SUPABASE_URL = os.getenv("SUPABASE_URL", "https://tzwepclhllftfmoeimjd.supabase.co")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

if not SUPABASE_SERVICE_KEY:
    raise RuntimeError("Thi·∫øu SUPABASE_SERVICE_KEY trong ENV")

REST_BASE_URL = f"{SUPABASE_URL}/rest/v1"
STORAGE_BASE_URL = f"{SUPABASE_URL}/storage/v1"

COMMON_HEADERS = {
    "apikey": SUPABASE_SERVICE_KEY,
    "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
}

JSON_HEADERS = {
    **COMMON_HEADERS,
    "Content-Type": "application/json",
    "Prefer": "return=minimal"
}


def df_to_records(df: pd.DataFrame):
    """Convert DataFrame to list[dict], rename CP -> ticker, NƒÉm -> year."""
    # Rename columns
    df = df.copy()
    if "CP" in df.columns:
        df.rename(columns={"CP": "ticker"}, inplace=True)
    if "NƒÉm" in df.columns:
        df.rename(columns={"NƒÉm": "year"}, inplace=True)
    
    # Convert NaN to None for JSON serialization
    df_clean = df.where(pd.notnull(df), None)
    return df_clean.to_dict(orient="records")


def upsert_table(df: pd.DataFrame, table_name: str, chunk_size: int = 300):
    """G·ª≠i d·ªØ li·ªáu l√™n Supabase REST API theo t·ª´ng chunk."""
    records = df_to_records(df)
    print(f"üîπ Upsert {len(records)} rows v√†o b·∫£ng {table_name} qua REST API...")

    url = f"{REST_BASE_URL}/{table_name}"

    for i in range(0, len(records), chunk_size):
        chunk = records[i:i + chunk_size]
        resp = requests.post(url, headers=JSON_HEADERS, data=json.dumps(chunk))
        if not resp.ok:
            print(f"‚ùå L·ªói khi upsert chunk {i}-{i+len(chunk)} v√†o {table_name}: {resp.status_code}")
            print(resp.text)
            resp.raise_for_status()
        else:
            print(f"‚úÖ ƒê√£ upsert {len(chunk)} rows v√†o {table_name}")


def upload_to_storage(local_path: str, remote_path: str, bucket: str = "processed-data"):
    """Upload file l√™n Supabase Storage qua REST API."""
    url = f"{STORAGE_BASE_URL}/object/{bucket}/{remote_path}"
    params = {"upsert": "true"}

    ext = os.path.splitext(local_path)[1].lower()
    content_type = "text/csv" if ext == ".csv" else "application/octet-stream"

    headers = {
        **COMMON_HEADERS,
        "Content-Type": content_type,
    }

    with open(local_path, "rb") as f:
        resp = requests.post(url, headers=headers, params=params, data=f)
        
        if resp.status_code == 403:
            print(f"‚ö†Ô∏è  RLS policy blocked, retrying with delete first...")
            delete_url = f"{STORAGE_BASE_URL}/object/{bucket}/{remote_path}"
            requests.delete(delete_url, headers=COMMON_HEADERS)
            
            f.seek(0)
            resp = requests.post(url, headers=headers, params=params, data=f)
        
        if not resp.ok:
            print(f"‚ùå L·ªói upload {local_path} -> {bucket}/{remote_path}: {resp.status_code}")
            print(resp.text)
            print(f"‚ö†Ô∏è  L·ªói storage nh∆∞ng DB ƒë√£ ok, ti·∫øp t·ª•c...")
        else:
            print(f"‚úÖ Uploaded {local_path} -> {bucket}/{remote_path}")


def create_table_if_not_exists(df: pd.DataFrame, table_name: str):
    """T·ª± ƒë·ªông t·∫°o b·∫£ng tr√™n Supabase n·∫øu ch∆∞a t·ªìn t·∫°i."""
    # Rename columns
    df = df.copy()
    if "CP" in df.columns:
        df.rename(columns={"CP": "ticker"}, inplace=True)
    if "NƒÉm" in df.columns:
        df.rename(columns={"NƒÉm": "year"}, inplace=True)
    
    sql = f"CREATE TABLE IF NOT EXISTS {table_name} (\n"
    sql += "  id bigserial primary key,\n"
    
    for col in df.columns:
        # Determine type based on first non-null value
        dtype = "numeric"  # Default for financial data
        if col in ["ticker"]:
            dtype = "text"
        elif col in ["year"]:
            dtype = "int"
        
        # Escape column names v·ªõi quotes
        col_escaped = f'"{col}"' if col not in ["id", "ticker", "year"] else col
        sql += f"  {col_escaped} {dtype},\n"
    
    sql += "  created_at timestamp default now()\n"
    sql += ");\n"
    
    # Execute SQL qua REST API
    url = f"{REST_BASE_URL}/rest/v1/rpc/exec_sql"
    headers = {
        **COMMON_HEADERS,
        "Content-Type": "application/json",
    }
    
    payload = {"query": sql}
    
    try:
        resp = requests.post(url, headers=headers, json=payload)
        if resp.ok:
            print(f"‚úÖ B·∫£ng {table_name} ƒë√£ s·∫µn s√†ng")
        else:
            # N·∫øu endpoint kh√¥ng t·ªìn t·∫°i, th·ª≠ c√°ch kh√°c: exec via postgrest
            print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ auto-create b·∫£ng {table_name}, vui l√≤ng t·∫°o manual ho·∫∑c ki·ªÉm tra b·∫£ng t·ªìn t·∫°i")
            print(f"SQL: {sql}")
    except Exception as e:
        print(f"‚ö†Ô∏è  L·ªói khi t·∫°o b·∫£ng {table_name}: {e}")


def run_etl():
    # 1) EXTRACT
    print("üîπ Extract: d√πng VNStock ƒë·ªÉ l·∫•y b√°o c√°o t√†i ch√≠nh FPT...")
    
    stock = Vnstock().stock(symbol="FPT", source="VCI")
    
    income_df = stock.finance.income_statement(period="year", lang="vi", dropna=True)
    balance_df = stock.finance.balance_sheet(period="year", lang="vi", dropna=True)
    cashflow_df = stock.finance.cash_flow(period="year", dropna=True)

    print("‚û° Income Statement sample:")
    print(income_df.head())
    print(f"\nColumns ({len(income_df.columns)}): {income_df.columns.tolist()}")

    # 2) AUTO CREATE TABLES n·∫øu ch∆∞a t·ªìn t·∫°i
    print("\nüîπ Auto-create tables n·∫øu ch∆∞a t·ªìn t·∫°i...")
    create_table_if_not_exists(income_df, "fpt_income_statement")
    create_table_if_not_exists(balance_df, "fpt_balance_sheet")
    create_table_if_not_exists(cashflow_df, "fpt_cash_flow")
    
    print("\n‚úÖ Tables ready!")

    # 3) TRANSFORM
    print("\nüîπ Transform: chu·∫©n h√≥a d·ªØ li·ªáu...")
    
    # Rename columns ƒë·ªÉ match DB schema
    income_df = income_df.copy()
    balance_df = balance_df.copy()
    cashflow_df = cashflow_df.copy()
    
    for df in [income_df, balance_df, cashflow_df]:
        if "CP" in df.columns:
            df.rename(columns={"CP": "ticker"}, inplace=True)
        if "NƒÉm" in df.columns:
            df.rename(columns={"NƒÉm": "year"}, inplace=True)
    
    print(f"‚úÖ Renamed columns")
    print(f"üìä Income shape: {income_df.shape}")
    print(f"üìä Balance shape: {balance_df.shape}")
    print(f"üìä Cashflow shape: {cashflow_df.shape}")

    # L∆∞u CSV (original format)
    income_df.to_csv("income_statement.csv", index=False)
    balance_df.to_csv("balance_sheet.csv", index=False)
    cashflow_df.to_csv("cash_flow.csv", index=False)
    print("‚úÖ ƒê√£ l∆∞u 3 file CSV.")

    # 3) LOAD ‚Üí Supabase qua REST API
    print("\nüîπ Load v√†o Supabase...")
    upsert_table(income_df, "fpt_income_statement")
    upsert_table(balance_df, "fpt_balance_sheet")
    upsert_table(cashflow_df, "fpt_cash_flow")

    print("‚úÖ ƒê√£ g·ª≠i d·ªØ li·ªáu l√™n 3 b·∫£ng qua REST API.")

    # 4) UPLOAD CSV ‚Üí STORAGE
    print("\nüîπ Upload 3 file CSV l√™n bucket processed-data...")

    upload_to_storage("income_statement.csv", "income_statement.csv")
    upload_to_storage("balance_sheet.csv", "balance_sheet.csv")
    upload_to_storage("cash_flow.csv", "cash_flow.csv")

    print("\n‚úÖ ETL ho√†n t·∫•t!")


if __name__ == "__main__":
    run_etl()
