"""
ETL pipeline with Supabase REST API + VNStock:
- Extract 3 b√°o c√°o t√†i ch√≠nh t·ª´ VNStock
- Transform: pack d·ªØ li·ªáu v√†o c·ªôt data (JSONB)
- Load v√†o Supabase PostgreSQL qua REST API
"""

import os
import json
import pandas as pd
import requests
from vnstock import Vnstock

SUPABASE_URL = os.getenv("SUPABASE_URL", "https://tzwepclhllftfmoeimjd.supabase.co")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

if not SUPABASE_SERVICE_KEY:
    raise RuntimeError("Thi·∫øu SUPABASE_SERVICE_KEY trong ENV")

REST_BASE_URL = f"{SUPABASE_URL}/rest/v1"
STORAGE_BASE_URL = f"{SUPABASE_URL}/storage/v1"

COMMON_HEADERS = {
    "apikey": SUPABASE_SERVICE_KEY,
    "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
}

JSON_HEADERS = {
    **COMMON_HEADERS,
    "Content-Type": "application/json",
    "Prefer": "return=minimal"
}


def df_to_jsonb_records(df: pd.DataFrame):
    """
    Convert DataFrame to records with JSONB format.
    Gi·∫£ s·ª≠ DataFrame c√≥ c·ªôt 'NƒÉm' ho·∫∑c 'Year', c√°c c·ªôt kh√°c pack v√†o 'data'.
    """
    records = []
    
    # T√¨m c·ªôt nƒÉm (NƒÉm ho·∫∑c Year)
    year_col = None
    for col in df.columns:
        if col.lower() in ['nƒÉm', 'year']:
            year_col = col
            break
    
    for _, row in df.iterrows():
        year = int(row[year_col]) if year_col and pd.notna(row[year_col]) else None
        
        # Pack to√†n b·ªô d·ªØ li·ªáu v√†o JSONB
        data_dict = {}
        for col in df.columns:
            if col.lower() not in ['nƒÉm', 'year', 'cp', 'ticker']:
                val = row[col]
                data_dict[col] = None if pd.isna(val) else val
        
        ticker = row.get('CP') or row.get('ticker', 'FPT')
        
        record = {
            "ticker": ticker,
            "year": year,
            "data": data_dict
        }
        records.append(record)
    
    return records


def upsert_table(records: list, table_name: str, chunk_size: int = 300):
    """G·ª≠i d·ªØ li·ªáu l√™n Supabase REST API theo t·ª´ng chunk."""
    print(f"üîπ Upsert {len(records)} rows v√†o b·∫£ng {table_name} qua REST API...")

    url = f"{REST_BASE_URL}/{table_name}"

    for i in range(0, len(records), chunk_size):
        chunk = records[i:i + chunk_size]
        resp = requests.post(url, headers=JSON_HEADERS, data=json.dumps(chunk))
        if not resp.ok:
            print(f"‚ùå L·ªói khi upsert chunk {i}-{i+len(chunk)} v√†o {table_name}: {resp.status_code}")
            print(resp.text)
            resp.raise_for_status()
        else:
            print(f"‚úÖ ƒê√£ upsert {len(chunk)} rows v√†o {table_name}")


def upload_to_storage(local_path: str, remote_path: str, bucket: str = "processed-data"):
    """Upload file l√™n Supabase Storage qua REST API."""
    url = f"{STORAGE_BASE_URL}/object/{bucket}/{remote_path}"
    params = {"upsert": "true"}

    ext = os.path.splitext(local_path)[1].lower()
    content_type = "text/csv" if ext == ".csv" else "application/octet-stream"

    headers = {
        **COMMON_HEADERS,
        "Content-Type": content_type,
    }

    with open(local_path, "rb") as f:
        resp = requests.post(url, headers=headers, params=params, data=f)
        if not resp.ok:
            print(f"‚ùå L·ªói upload {local_path} -> {bucket}/{remote_path}: {resp.status_code}")
            print(resp.text)
            resp.raise_for_status()
        else:
            print(f"‚úÖ Uploaded {local_path} -> {bucket}/{remote_path}")


def run_etl():
    # 1) EXTRACT
    print("üîπ Extract: d√πng VNStock ƒë·ªÉ l·∫•y b√°o c√°o t√†i ch√≠nh FPT...")
    
    stock = Vnstock().stock(symbol="FPT", source="VCI")
    
    income_df = stock.finance.income_statement(period="year", lang="vi", dropna=True)
    balance_df = stock.finance.balance_sheet(period="year", lang="vi", dropna=True)
    cashflow_df = stock.finance.cash_flow(period="year", dropna=True)

    print("‚û° Income Statement sample:")
    print(income_df.head())
    print(f"Columns: {income_df.columns.tolist()}")

    # 2) TRANSFORM
    print("üîπ Transform: chu·∫©n h√≥a d·ªØ li·ªáu...")
    
    income_records = df_to_jsonb_records(income_df)
    balance_records = df_to_jsonb_records(balance_df)
    cashflow_records = df_to_jsonb_records(cashflow_df)
    
    print(f"‚úÖ Converted {len(income_records)} income records")
    print(f"‚úÖ Converted {len(balance_records)} balance records")
    print(f"‚úÖ Converted {len(cashflow_records)} cashflow records")
    
    print(f"\nüìã Sample income record: {json.dumps(income_records[0], ensure_ascii=False, indent=2)}")

    # L∆∞u CSV (original format)
    income_df.to_csv("income_statement.csv", index=False)
    balance_df.to_csv("balance_sheet.csv", index=False)
    cashflow_df.to_csv("cash_flow.csv", index=False)
    print("‚úÖ ƒê√£ l∆∞u 3 file CSV.")

    # 3) LOAD ‚Üí Supabase qua REST API
    upsert_table(income_records, "fpt_income_statement")
    upsert_table(balance_records, "fpt_balance_sheet")
    upsert_table(cashflow_records, "fpt_cash_flow")

    print("‚úÖ ƒê√£ g·ª≠i d·ªØ li·ªáu l√™n 3 b·∫£ng qua REST API.")

    # 4) UPLOAD CSV ‚Üí STORAGE
    print("üîπ Upload 3 file CSV l√™n bucket processed-data...")

    upload_to_storage("income_statement.csv", "income_statement.csv")
    upload_to_storage("balance_sheet.csv", "balance_sheet.csv")
    upload_to_storage("cash_flow.csv", "cash_flow.csv")

    print("‚úÖ ETL ho√†n t·∫•t!")


if __name__ == "__main__":
    run_etl()

